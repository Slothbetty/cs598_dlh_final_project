{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bzhao10/cs598_dlh_final_project/blob/main/model_without_cat_rolled_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAOoKyInOiNv"
      },
      "source": [
        "CS 598 Final Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkz1rde_PJrT",
        "outputId": "c72f87b9-833d-4ec6-9b09-45a570d7631c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8kUV5_djOg7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1d2658-9eca-4706-f9b7-b5672cd002ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Unnamed: 0                                               TEXT  \\\n",
            "0             41  ccu nursing progress notepls see transfer summ...   \n",
            "1             42  nursing progress notes month only i have some ...   \n",
            "2             43  respiratory care pt extubated to a aerosol avr...   \n",
            "3             44  respiratory care patient remains on cpap psv v...   \n",
            "4             45  repeat k hct due post blood transfusion lactol...   \n",
            "...          ...                                                ...   \n",
            "9995       10888  title case management initial assessment copy ...   \n",
            "9996       13175  discharge plan this nurse case manager was not...   \n",
            "9997       16260  case management discharge planning note this n...   \n",
            "9998        4889  title cardiology follow up events patient note...   \n",
            "9999        3652  pharmacy note digibind dose assessment pharmac...   \n",
            "\n",
            "      rolled_up_code_008  rolled_up_code_009  rolled_up_code_010  \\\n",
            "0                      0                   0                   0   \n",
            "1                      0                   0                   0   \n",
            "2                      0                   0                   0   \n",
            "3                      0                   0                   0   \n",
            "4                      0                   0                   0   \n",
            "...                  ...                 ...                 ...   \n",
            "9995                   0                   0                   0   \n",
            "9996                   0                   0                   0   \n",
            "9997                   0                   0                   0   \n",
            "9998                   0                   0                   0   \n",
            "9999                   0                   0                   0   \n",
            "\n",
            "      rolled_up_code_038  rolled_up_code_041  rolled_up_code_042  \\\n",
            "0                      0                   0                   0   \n",
            "1                      0                   0                   0   \n",
            "2                      0                   0                   0   \n",
            "3                      0                   0                   0   \n",
            "4                      0                   0                   0   \n",
            "...                  ...                 ...                 ...   \n",
            "9995                   1                   0                   1   \n",
            "9996                   0                   1                   0   \n",
            "9997                   0                   1                   0   \n",
            "9998                   0                   0                   0   \n",
            "9999                   1                   0                   0   \n",
            "\n",
            "      rolled_up_code_053  rolled_up_code_070  ...  category_Case Management  \\\n",
            "0                      0                   0  ...                         0   \n",
            "1                      0                   0  ...                         0   \n",
            "2                      0                   0  ...                         0   \n",
            "3                      0                   0  ...                         0   \n",
            "4                      0                   0  ...                         0   \n",
            "...                  ...                 ...  ...                       ...   \n",
            "9995                   0                   1  ...                         1   \n",
            "9996                   0                   0  ...                         1   \n",
            "9997                   0                   0  ...                         1   \n",
            "9998                   0                   0  ...                         0   \n",
            "9999                   0                   0  ...                         0   \n",
            "\n",
            "      category_Consult  category_General  category_Nursing/other  \\\n",
            "0                    0                 0                       1   \n",
            "1                    0                 0                       1   \n",
            "2                    0                 0                       1   \n",
            "3                    0                 0                       1   \n",
            "4                    0                 0                       1   \n",
            "...                ...               ...                     ...   \n",
            "9995                 0                 0                       0   \n",
            "9996                 0                 0                       0   \n",
            "9997                 0                 0                       0   \n",
            "9998                 1                 0                       0   \n",
            "9999                 0                 0                       0   \n",
            "\n",
            "      category_Nutrition  category_Pharmacy  category_Physician  \\\n",
            "0                      0                  0                   0   \n",
            "1                      0                  0                   0   \n",
            "2                      0                  0                   0   \n",
            "3                      0                  0                   0   \n",
            "4                      0                  0                   0   \n",
            "...                  ...                ...                 ...   \n",
            "9995                   0                  0                   0   \n",
            "9996                   0                  0                   0   \n",
            "9997                   0                  0                   0   \n",
            "9998                   0                  0                   0   \n",
            "9999                   0                  1                   0   \n",
            "\n",
            "      category_Rehab Services  category_Respiratory  category_Social Work  \n",
            "0                           0                     0                     0  \n",
            "1                           0                     0                     0  \n",
            "2                           0                     0                     0  \n",
            "3                           0                     0                     0  \n",
            "4                           0                     0                     0  \n",
            "...                       ...                   ...                   ...  \n",
            "9995                        0                     0                     0  \n",
            "9996                        0                     0                     0  \n",
            "9997                        0                     0                     0  \n",
            "9998                        0                     0                     0  \n",
            "9999                        0                     0                     0  \n",
            "\n",
            "[10000 rows x 374 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "notes=pd.read_csv(\"/content/drive/MyDrive/DLH_final_project_dataset/preprocessed_rolled_up_icd_codes_10000.csv\")\n",
        "\n",
        "print(notes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rolled_up_codes_list = notes.loc[:, notes.columns.str.startswith('rolled_up_code_')].columns.tolist()\n",
        "print(rolled_up_codes_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcP9vr8rdS-z",
        "outputId": "2dd4e679-8a13-42ba-fbf8-9e61cdc24e63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rolled_up_code_008', 'rolled_up_code_009', 'rolled_up_code_010', 'rolled_up_code_038', 'rolled_up_code_041', 'rolled_up_code_042', 'rolled_up_code_053', 'rolled_up_code_070', 'rolled_up_code_078', 'rolled_up_code_079', 'rolled_up_code_112', 'rolled_up_code_117', 'rolled_up_code_135', 'rolled_up_code_154', 'rolled_up_code_155', 'rolled_up_code_156', 'rolled_up_code_157', 'rolled_up_code_162', 'rolled_up_code_171', 'rolled_up_code_172', 'rolled_up_code_174', 'rolled_up_code_193', 'rolled_up_code_197', 'rolled_up_code_198', 'rolled_up_code_201', 'rolled_up_code_202', 'rolled_up_code_203', 'rolled_up_code_204', 'rolled_up_code_205', 'rolled_up_code_208', 'rolled_up_code_209', 'rolled_up_code_211', 'rolled_up_code_214', 'rolled_up_code_215', 'rolled_up_code_218', 'rolled_up_code_225', 'rolled_up_code_227', 'rolled_up_code_228', 'rolled_up_code_238', 'rolled_up_code_241', 'rolled_up_code_242', 'rolled_up_code_244', 'rolled_up_code_250', 'rolled_up_code_252', 'rolled_up_code_253', 'rolled_up_code_254', 'rolled_up_code_255', 'rolled_up_code_258', 'rolled_up_code_259', 'rolled_up_code_261', 'rolled_up_code_263', 'rolled_up_code_266', 'rolled_up_code_268', 'rolled_up_code_272', 'rolled_up_code_274', 'rolled_up_code_275', 'rolled_up_code_276', 'rolled_up_code_277', 'rolled_up_code_278', 'rolled_up_code_279', 'rolled_up_code_280', 'rolled_up_code_281', 'rolled_up_code_282', 'rolled_up_code_284', 'rolled_up_code_285', 'rolled_up_code_286', 'rolled_up_code_287', 'rolled_up_code_288', 'rolled_up_code_289', 'rolled_up_code_290', 'rolled_up_code_291', 'rolled_up_code_292', 'rolled_up_code_293', 'rolled_up_code_294', 'rolled_up_code_295', 'rolled_up_code_296', 'rolled_up_code_298', 'rolled_up_code_300', 'rolled_up_code_301', 'rolled_up_code_303', 'rolled_up_code_305', 'rolled_up_code_306', 'rolled_up_code_309', 'rolled_up_code_311', 'rolled_up_code_318', 'rolled_up_code_327', 'rolled_up_code_331', 'rolled_up_code_332', 'rolled_up_code_333', 'rolled_up_code_336', 'rolled_up_code_338', 'rolled_up_code_340', 'rolled_up_code_342', 'rolled_up_code_343', 'rolled_up_code_345', 'rolled_up_code_348', 'rolled_up_code_349', 'rolled_up_code_350', 'rolled_up_code_351', 'rolled_up_code_353', 'rolled_up_code_355', 'rolled_up_code_356', 'rolled_up_code_357', 'rolled_up_code_358', 'rolled_up_code_359', 'rolled_up_code_362', 'rolled_up_code_365', 'rolled_up_code_366', 'rolled_up_code_369', 'rolled_up_code_372', 'rolled_up_code_378', 'rolled_up_code_379', 'rolled_up_code_386', 'rolled_up_code_389', 'rolled_up_code_394', 'rolled_up_code_396', 'rolled_up_code_397', 'rolled_up_code_398', 'rolled_up_code_401', 'rolled_up_code_402', 'rolled_up_code_403', 'rolled_up_code_410', 'rolled_up_code_411', 'rolled_up_code_412', 'rolled_up_code_413', 'rolled_up_code_414', 'rolled_up_code_415', 'rolled_up_code_416', 'rolled_up_code_420', 'rolled_up_code_421', 'rolled_up_code_423', 'rolled_up_code_424', 'rolled_up_code_425', 'rolled_up_code_426', 'rolled_up_code_427', 'rolled_up_code_428', 'rolled_up_code_429', 'rolled_up_code_430', 'rolled_up_code_431', 'rolled_up_code_432', 'rolled_up_code_433', 'rolled_up_code_434', 'rolled_up_code_435', 'rolled_up_code_437', 'rolled_up_code_438', 'rolled_up_code_440', 'rolled_up_code_441', 'rolled_up_code_442', 'rolled_up_code_443', 'rolled_up_code_444', 'rolled_up_code_447', 'rolled_up_code_451', 'rolled_up_code_452', 'rolled_up_code_453', 'rolled_up_code_454', 'rolled_up_code_455', 'rolled_up_code_456', 'rolled_up_code_458', 'rolled_up_code_459', 'rolled_up_code_462', 'rolled_up_code_464', 'rolled_up_code_465', 'rolled_up_code_473', 'rolled_up_code_478', 'rolled_up_code_482', 'rolled_up_code_484', 'rolled_up_code_485', 'rolled_up_code_486', 'rolled_up_code_487', 'rolled_up_code_490', 'rolled_up_code_491', 'rolled_up_code_492', 'rolled_up_code_493', 'rolled_up_code_496', 'rolled_up_code_501', 'rolled_up_code_507', 'rolled_up_code_510', 'rolled_up_code_511', 'rolled_up_code_512', 'rolled_up_code_515', 'rolled_up_code_516', 'rolled_up_code_518', 'rolled_up_code_519', 'rolled_up_code_523', 'rolled_up_code_525', 'rolled_up_code_527', 'rolled_up_code_530', 'rolled_up_code_531', 'rolled_up_code_532', 'rolled_up_code_533', 'rolled_up_code_535', 'rolled_up_code_536', 'rolled_up_code_537', 'rolled_up_code_550', 'rolled_up_code_552', 'rolled_up_code_553', 'rolled_up_code_555', 'rolled_up_code_556', 'rolled_up_code_557', 'rolled_up_code_560', 'rolled_up_code_562', 'rolled_up_code_564', 'rolled_up_code_565', 'rolled_up_code_566', 'rolled_up_code_567', 'rolled_up_code_568', 'rolled_up_code_569', 'rolled_up_code_570', 'rolled_up_code_571', 'rolled_up_code_572', 'rolled_up_code_573', 'rolled_up_code_574', 'rolled_up_code_575', 'rolled_up_code_576', 'rolled_up_code_577', 'rolled_up_code_578', 'rolled_up_code_579', 'rolled_up_code_581', 'rolled_up_code_583', 'rolled_up_code_584', 'rolled_up_code_585', 'rolled_up_code_587', 'rolled_up_code_588', 'rolled_up_code_590', 'rolled_up_code_591', 'rolled_up_code_592', 'rolled_up_code_593', 'rolled_up_code_594', 'rolled_up_code_596', 'rolled_up_code_598', 'rolled_up_code_599', 'rolled_up_code_600', 'rolled_up_code_607', 'rolled_up_code_608', 'rolled_up_code_619', 'rolled_up_code_623', 'rolled_up_code_626', 'rolled_up_code_648', 'rolled_up_code_681', 'rolled_up_code_682', 'rolled_up_code_686', 'rolled_up_code_692', 'rolled_up_code_693', 'rolled_up_code_695', 'rolled_up_code_696', 'rolled_up_code_707', 'rolled_up_code_711', 'rolled_up_code_712', 'rolled_up_code_713', 'rolled_up_code_714', 'rolled_up_code_715', 'rolled_up_code_716', 'rolled_up_code_719', 'rolled_up_code_721', 'rolled_up_code_722', 'rolled_up_code_723', 'rolled_up_code_724', 'rolled_up_code_725', 'rolled_up_code_728', 'rolled_up_code_729', 'rolled_up_code_730', 'rolled_up_code_731', 'rolled_up_code_733', 'rolled_up_code_737', 'rolled_up_code_745', 'rolled_up_code_746', 'rolled_up_code_747', 'rolled_up_code_753', 'rolled_up_code_755', 'rolled_up_code_780', 'rolled_up_code_781', 'rolled_up_code_782', 'rolled_up_code_783', 'rolled_up_code_784', 'rolled_up_code_785', 'rolled_up_code_786', 'rolled_up_code_787', 'rolled_up_code_788', 'rolled_up_code_789', 'rolled_up_code_790', 'rolled_up_code_791', 'rolled_up_code_792', 'rolled_up_code_794', 'rolled_up_code_799', 'rolled_up_code_800', 'rolled_up_code_801', 'rolled_up_code_802', 'rolled_up_code_805', 'rolled_up_code_807', 'rolled_up_code_808', 'rolled_up_code_810', 'rolled_up_code_812', 'rolled_up_code_820', 'rolled_up_code_821', 'rolled_up_code_823', 'rolled_up_code_834', 'rolled_up_code_839', 'rolled_up_code_852', 'rolled_up_code_864', 'rolled_up_code_865', 'rolled_up_code_867', 'rolled_up_code_873', 'rolled_up_code_875', 'rolled_up_code_882', 'rolled_up_code_893', 'rolled_up_code_905', 'rolled_up_code_908', 'rolled_up_code_909', 'rolled_up_code_920', 'rolled_up_code_933', 'rolled_up_code_934', 'rolled_up_code_942', 'rolled_up_code_952', 'rolled_up_code_958', 'rolled_up_code_965', 'rolled_up_code_969', 'rolled_up_code_995', 'rolled_up_code_996', 'rolled_up_code_997', 'rolled_up_code_998', 'rolled_up_code_999', 'rolled_up_code_E81', 'rolled_up_code_E84', 'rolled_up_code_E85', 'rolled_up_code_E86', 'rolled_up_code_E87', 'rolled_up_code_E88', 'rolled_up_code_E91', 'rolled_up_code_E92', 'rolled_up_code_E93', 'rolled_up_code_E94', 'rolled_up_code_E95', 'rolled_up_code_E98', 'rolled_up_code_V02', 'rolled_up_code_V03', 'rolled_up_code_V08', 'rolled_up_code_V09', 'rolled_up_code_V10', 'rolled_up_code_V12', 'rolled_up_code_V13', 'rolled_up_code_V14', 'rolled_up_code_V15', 'rolled_up_code_V16', 'rolled_up_code_V17', 'rolled_up_code_V18', 'rolled_up_code_V42', 'rolled_up_code_V43', 'rolled_up_code_V44', 'rolled_up_code_V45', 'rolled_up_code_V46', 'rolled_up_code_V49', 'rolled_up_code_V53', 'rolled_up_code_V54', 'rolled_up_code_V55', 'rolled_up_code_V58', 'rolled_up_code_V62', 'rolled_up_code_V64', 'rolled_up_code_V66', 'rolled_up_code_V70', 'rolled_up_code_V85', 'rolled_up_code_V87', 'rolled_up_code_V88']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "class Preprocessing:\n",
        "  def __init__(self, num_words, seq_len, target, binary=True):\n",
        "    self.data = '/content/drive/MyDrive/DLH_final_project_dataset/preprocessed_rolled_up_icd_codes_10000.csv'\n",
        "    self.num_words = num_words\n",
        "    self.seq_len = seq_len\n",
        "    self.vocabulary = None\n",
        "    self.x_tokenized = None\n",
        "    self.x_padded = None\n",
        "    self.x_raw = None\n",
        "    self.y = None\n",
        "    \n",
        "    self.x_train = None\n",
        "    self.x_test = None\n",
        "    self.y_train = None\n",
        "    self.y_test = None\n",
        "\n",
        "    self.target = target\n",
        "    \n",
        "  def load_data(self, binary):\n",
        "    # Reads the raw csv file and split into\n",
        "    # sentences (x) and target (y)\n",
        "    df = pd.read_csv(self.data)\n",
        "    self.x_raw = df['TEXT'].values\n",
        "    if (binary==False):\n",
        "      #df['rolled_up_codes'] = df.loc[:, rolled_up_codes_list[0]:rolled_up_codes_list[len(rolled_up_codes_list)-1]].values.tolist()\n",
        "      df['rolled_up_codes'] = df.loc[:, 'rolled_up_code_008':'rolled_up_code_V88'].values.tolist()\n",
        "      self.y = df['rolled_up_codes'].values\n",
        "    else:\n",
        "      self.y = df[self.target].values\n",
        "    \n",
        "  def text_tokenization(self):\n",
        "    # Tokenizes each sentence by implementing the nltk tool\n",
        "    self.x_raw = [word_tokenize(x) for x in self.x_raw]\n",
        "    \n",
        "  def build_vocabulary(self):\n",
        "    # Builds the vocabulary and keeps the \"x\" most frequent word\n",
        "    self.vocabulary = dict()\n",
        "    fdist = nltk.FreqDist()\n",
        "    \n",
        "    for sentence in self.x_raw:\n",
        "      for word in sentence:\n",
        "        fdist[word] += 1\n",
        "        \n",
        "    common_words = fdist.most_common(self.num_words)\n",
        "    \n",
        "    for idx, word in enumerate(common_words):\n",
        "      self.vocabulary[word[0]] = (idx+1)\n",
        "  \n",
        "  def word_to_idx(self):\n",
        "    # By using the dictionary (vocabulary), it is transformed\n",
        "    # each token into its index based representatio\n",
        "    self.x_tokenized = list() \n",
        "    \n",
        "    for sentence in self.x_raw:\n",
        "      temp_sentence = list()\n",
        "      for word in sentence:\n",
        "        if word in self.vocabulary.keys():\n",
        "          temp_sentence.append(self.vocabulary[word])\n",
        "      self.x_tokenized.append(temp_sentence)\n",
        "      \n",
        "  def padding_sentences(self):\n",
        "    # Each sentence which does not fulfill the required le\n",
        "    # it's padded with the index 0\n",
        "    pad_idx = 0\n",
        "    self.x_padded = list()\n",
        "    \n",
        "    for sentence in self.x_tokenized:\n",
        "      while len(sentence) < self.seq_len:\n",
        "        sentence.insert(len(sentence), pad_idx)\n",
        "      self.x_padded.append(sentence)\n",
        "      \n",
        "    self.x_padded = np.array(self.x_padded)\n",
        "    \n",
        "  def split_data(self):\n",
        "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x_padded, self.y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dixeG9PweDHJ",
        "outputId": "31c5161e-3b52-473f-e271-ceea0761dadf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of Trick's model for binary prediction\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class BoT(nn.ModuleList):\n",
        "\n",
        "   def __init__(self, seq_len=2200,num_words=45173,embedding_size=64,out_size=32):\n",
        "      super(BoT, self).__init__()\n",
        "\n",
        "      # Parameters regarding text preprocessing\n",
        "      self.seq_len = seq_len\n",
        "      self.num_words = num_words\n",
        "      self.embedding_size = embedding_size\n",
        "  \n",
        "      # Kernel sizes\n",
        "      self.kernel_1 = 3\n",
        "   \n",
        "      # Output size for each convolution\n",
        "      self.out_size = out_size\n",
        "      \n",
        "      # Embedding layer definition\n",
        "      self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
        "      \n",
        "      # Fully connected layer definition\n",
        "      self.fc = nn.Linear(64, 1)\n",
        "\n",
        "   def forward(self, x):\n",
        "\n",
        "      x = self.embedding(x)\n",
        "      #x = [batch size, seq. length, hidden dim]\n",
        "      \n",
        "      x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze(1) #averages word vectors across whole sequence length\n",
        "      #x = [batch size, hidden dim]\n",
        "\n",
        "      x = self.fc(x)\n",
        "      #x = [batch size, output dim]\n",
        "\n",
        "      # Activation function is applied\n",
        "      x = torch.sigmoid(x)\n",
        "      \n",
        "      return x.squeeze()"
      ],
      "metadata": {
        "id": "dwpiiMrZfZyH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of Trick's model for multi-label prediction\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class BoT_mul(nn.ModuleList):\n",
        "\n",
        "   def __init__(self, seq_len=2200,num_words=45173, labels=362, embedding_size=64,out_size=32):\n",
        "      super(BoT_mul, self).__init__()\n",
        "\n",
        "      # Parameters regarding text preprocessing\n",
        "      self.seq_len = seq_len\n",
        "      self.num_words = num_words\n",
        "      self.embedding_size = embedding_size\n",
        "  \n",
        "      # Kernel sizes\n",
        "      self.kernel_1 = 3\n",
        "   \n",
        "      # Output size for each convolution\n",
        "      self.out_size = out_size\n",
        "      \n",
        "      # Embedding layer definition\n",
        "      self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
        "      \n",
        "      # Fully connected layer definition\n",
        "      self.fc = nn.Linear(64, labels)\n",
        "\n",
        "   def forward(self, x):\n",
        "\n",
        "      x = self.embedding(x)\n",
        "      #x = [batch size, seq. length, hidden dim]\n",
        "      \n",
        "      x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze(1) #averages word vectors across whole sequence length\n",
        "      #x = [batch size, hidden dim]\n",
        "\n",
        "      x = self.fc(x)\n",
        "      #x = [batch size, output dim]\n",
        "\n",
        "      # Activation function is applied\n",
        "      x = torch.sigmoid(x)\n",
        "      \n",
        "      return x.squeeze()"
      ],
      "metadata": {
        "id": "dzYlXqQJVpua"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN baseline model for binary prediction\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class CNN_baseline(nn.ModuleList):\n",
        "\n",
        "   def __init__(self, seq_len=2200,num_words=45173,embedding_size=64,out_size=32):\n",
        "      super(CNN_baseline, self).__init__()\n",
        "\n",
        "      # Parameters regarding text preprocessing\n",
        "      self.seq_len = seq_len\n",
        "      self.num_words = num_words\n",
        "      self.embedding_size = embedding_size\n",
        "      \n",
        "      # CNN parameters definition\n",
        "      # Kernel sizes\n",
        "      self.kernel_1 = 3\n",
        "\n",
        "  \n",
        "      # Output size for each convolution\n",
        "      self.out_size = out_size\n",
        "      \n",
        "      # Embedding layer definition\n",
        "      self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
        "      \n",
        "      # Convolution layers definition\n",
        "      self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1)\n",
        "      \n",
        "      # Max pooling layers definition\n",
        "      self.pool_1 = nn.MaxPool1d(self.kernel_1)\n",
        "      \n",
        "      # Fully connected layer definition\n",
        "      self.fc = nn.Linear(640, 1)\n",
        " \n",
        "   def forward(self, x):\n",
        "      \n",
        "      x = self.embedding(x)\n",
        "      #x = [batch size, seq. length, hidden dim]\n",
        "\n",
        "      # Convolution layer 1 is applied\n",
        "      x1 = self.conv_1(x)\n",
        "      x1 = torch.relu(x1)\n",
        "      x1 = self.pool_1(x1)\n",
        "      \n",
        "      # The output of each convolutional layer is concatenated into a unique vector\n",
        "      x1=torch.flatten(x1, 1)\n",
        "      # The \"flattened\" vector is passed through a fully connected layer\n",
        "      out = self.fc(x1)\n",
        "      # Activation function is applied\n",
        "      out = torch.sigmoid(out)\n",
        "      \n",
        "      return out.squeeze()"
      ],
      "metadata": {
        "id": "x9zXNWiMWV0k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_baseline_mul(nn.ModuleList):\n",
        "\n",
        "   def __init__(self, seq_len=2200,num_words=45173,labels=362,embedding_size=64,out_size=32):\n",
        "      super(CNN_baseline_mul, self).__init__()\n",
        "\n",
        "      # Parameters regarding text preprocessing\n",
        "      self.seq_len = seq_len\n",
        "      self.num_words = num_words\n",
        "      self.embedding_size = embedding_size\n",
        "      \n",
        "      # CNN parameters definition\n",
        "      # Kernel sizes\n",
        "      self.kernel_1 = 3\n",
        "\n",
        "      \n",
        "      # Output size for each convolution\n",
        "      self.out_size = out_size\n",
        "      \n",
        "      # Embedding layer definition\n",
        "      self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
        "      \n",
        "      # Convolution layers definition\n",
        "      self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1)\n",
        "      \n",
        "      # Max pooling layers definition\n",
        "      self.pool_1 = nn.MaxPool1d(self.kernel_1)\n",
        "      \n",
        "      # Fully connected layer definition\n",
        "      self.fc = nn.Linear(640, labels)\n",
        " \n",
        "   def forward(self, x):\n",
        "      \n",
        "      # Sequence of tokes is filterd through an embedding layer\n",
        "      x = self.embedding(x)\n",
        "\n",
        "      # Convolution layer 1 is applied\n",
        "      x1 = self.conv_1(x)\n",
        "      x1 = torch.relu(x1)\n",
        "      x1 = self.pool_1(x1)\n",
        "      \n",
        "      # The output of each convolutional layer is concatenated into a unique vector\n",
        "      x1=torch.flatten(x1, 1)\n",
        "      # The \"flattened\" vector is passed through a fully connected layer\n",
        "      out = self.fc(x1)\n",
        "      # Activation function is applied\n",
        "      out = torch.sigmoid(out)\n",
        "      \n",
        "      return out.squeeze()"
      ],
      "metadata": {
        "id": "LdbdC_wzVR7Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_Three_Conv_multi(nn.ModuleList):\n",
        "\n",
        "  def __init__(self,seq_len=2200,num_words=45173, labels= 362, embedding_size=64,out_size=32,stride=2):\n",
        "          super(CNN_Three_Conv_multi, self).__init__()\n",
        "\n",
        "          # Parameters regarding text preprocessing\n",
        "          self.seq_len = seq_len\n",
        "          self.num_words = num_words\n",
        "          self.embedding_size = embedding_size\n",
        "          \n",
        "          # Dropout definition\n",
        "          self.dropout = nn.Dropout(0.33)\n",
        "          \n",
        "          # CNN parameters definition\n",
        "          # Kernel sizes\n",
        "          self.kernel_1 = 2\n",
        "          self.kernel_2 = 3\n",
        "          self.kernel_3 = 4\n",
        "          \n",
        "          \n",
        "          # Output size for each convolution\n",
        "          self.out_size = out_size\n",
        "          # Number of strides for each convolution\n",
        "          self.stride = stride\n",
        "          \n",
        "          # Embedding layer definition\n",
        "          self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
        "          \n",
        "          # Convolution layers definition\n",
        "          self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
        "          self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
        "          self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
        "\n",
        "          \n",
        "          # Max pooling layers definition\n",
        "          self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
        "          self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
        "          self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
        "          \n",
        "          # Fully connected layer definition\n",
        "          self.fc = nn.Linear(self.in_features_fc(), labels)\n",
        "\n",
        "          \n",
        "  def in_features_fc(self):\n",
        "          '''Calculates the number of output features after Convolution + Max pooling\n",
        "                  \n",
        "          Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
        "          Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
        "          \n",
        "          source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
        "          '''\n",
        "          # Calcualte size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
        "          out_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
        "          out_conv_1 = math.floor(out_conv_1)\n",
        "          out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
        "          out_pool_1 = math.floor(out_pool_1)\n",
        "          \n",
        "          # Calcualte size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
        "          out_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
        "          out_conv_2 = math.floor(out_conv_2)\n",
        "          out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
        "          out_pool_2 = math.floor(out_pool_2)\n",
        "          \n",
        "          # Calcualte size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
        "          out_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
        "          out_conv_3 = math.floor(out_conv_3)\n",
        "          out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
        "          out_pool_3 = math.floor(out_pool_3)\n",
        "          \n",
        "          # Returns \"flattened\" vector (input for fully connected layer)\n",
        "          return (out_pool_1 + out_pool_2 + out_pool_3) * self.out_size      \n",
        "          \n",
        "  def forward(self, x):\n",
        "\n",
        "          # Sequence of tokes is filterd through an embedding layer\n",
        "          x = self.embedding(x)\n",
        "          \n",
        "          # Convolution layer 1 is applied\n",
        "          x1 = self.conv_1(x)\n",
        "          x1 = torch.relu(x1)\n",
        "          x1 = self.pool_1(x1)\n",
        "          \n",
        "          # Convolution layer 2 is applied\n",
        "          x2 = self.conv_2(x)\n",
        "          x2 = torch.relu((x2))\n",
        "          x2 = self.pool_2(x2)\n",
        "\n",
        "          # Convolution layer 3 is applied\n",
        "          x3 = self.conv_3(x)\n",
        "          x3 = torch.relu(x3)\n",
        "          x3 = self.pool_3(x3)\n",
        "          \n",
        "          \n",
        "          # The output of each convolutional layer is concatenated into a unique vector\n",
        "          union = torch.cat((x1, x2, x3), 2)\n",
        "          union = union.reshape(union.size(0), -1)\n",
        "\n",
        "          # The \"flattened\" vector is passed through a fully connected layer\n",
        "          out = self.fc(union)\n",
        "          # Dropout is applied        \n",
        "          out = self.dropout(out)\n",
        "          # Activation function is applied\n",
        "          out = torch.sigmoid(out)\n",
        "          \n",
        "          return out.squeeze()"
      ],
      "metadata": {
        "id": "WrmD1q5FOIn6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DatasetMaper(Dataset):\n",
        "\n",
        "\tdef __init__(self, x, y):\n",
        "\t\tself.x = x\n",
        "\t\tself.y = y\n",
        "\t\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.x)\n",
        "\t\t\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "ixHvhzCDnEwg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Run:\n",
        "    '''Training, evaluation and metrics calculation'''\n",
        "\n",
        "    @staticmethod\n",
        "    def train(model, data, multi=362, binary=True):\n",
        "        \n",
        "        # Initialize dataset maper\n",
        "        train = DatasetMaper(data['x_train'], data['y_train'])\n",
        "        test = DatasetMaper(data['x_test'], data['y_test'])\n",
        "        \n",
        "        # Initialize loaders\n",
        "        loader_train = DataLoader(train, batch_size=32)\n",
        "        loader_test = DataLoader(test, batch_size=32)\n",
        "        \n",
        "        # Define optimizer\n",
        "        optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "        \n",
        "        # Starts training phase\n",
        "        for epoch in range(15):\n",
        "            # Set model in training model\n",
        "            model.train()\n",
        "            predictions = []\n",
        "            # Starts batch training\n",
        "            for x_batch, y_batch in loader_train:\n",
        "                if (binary==False):\n",
        "                  y_batch = torch.stack(y_batch).T\n",
        "                y_batch = y_batch.type(torch.FloatTensor)\n",
        "            \n",
        "                # Feed the model\n",
        "                y_pred = model(x_batch)\n",
        "                \n",
        "                # Loss calculation\n",
        "                loss = F.binary_cross_entropy(y_pred, y_batch)\n",
        "                \n",
        "                # Clean gradientes\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # Gradients calculation\n",
        "                loss.backward()\n",
        "                \n",
        "                # Gradients update\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Save predictions\n",
        "                predictions += list(y_pred.detach().numpy())\n",
        "            \n",
        "            # Evaluation phase\n",
        "            test_predictions = Run.evaluation(model, loader_test)\n",
        "            if (binary==True):\n",
        "              predictions=[1 if pred>0.5 else 0 for pred in predictions]\n",
        "\n",
        "              test_predictions=[1 if pred>0.5 else 0 for pred in test_predictions]\n",
        "\n",
        "              train_precision,train_recall,train_f1,_ = precision_recall_fscore_support(predictions,data['y_train'],average='binary')\n",
        "              test_precision,test_recall,test_f1,_ = precision_recall_fscore_support(test_predictions,data['y_test'],average='binary')\n",
        "              test_accuracy=accuracy_score(data['y_test'],test_predictions)\n",
        "              print(\"Epoch: %d, loss: %.5f, Train precision: %.5f,Train recall: %.5f,Train f1: %.5f, Test accuracy: %.5f,Test precision: %.5f, Test recall: %.5f,Test f1: %.5f\" % (epoch+1, loss.item(), train_precision,train_recall,train_f1,test_accuracy,test_precision,test_recall,test_f1))\n",
        "            else:\n",
        "              for i in range(len(predictions)):\n",
        "                predictions[i]=[1 if pred>0.5 else 0 for pred in predictions[i]]  \n",
        "\n",
        "              for i in range(len(test_predictions)):\n",
        "                test_predictions[i]=[1 if pred>0.5 else 0 for pred in test_predictions[i]] \n",
        "              \n",
        "              multi_label_predictions = np.array(predictions)\n",
        "              y_train=np.empty([7500,multi])\n",
        "              for i in range(len(data['y_train'])):\n",
        "                y_train[i][:]=data['y_train'][i]\n",
        "\n",
        "              train_precision_all = np.empty([multi])\n",
        "              train_recall_all = np.empty([multi])\n",
        "              train_f1_all = np.empty([multi])\n",
        "              for i in range(len(predictions[0])):\n",
        "                  train_precision_all[i],train_recall_all[i],train_f1_all[i],_ = precision_recall_fscore_support(multi_label_predictions[:][i],y_train[:][i],average='binary')\n",
        "              train_precision = train_precision_all.mean()\n",
        "              train_recall = train_recall_all.mean()\n",
        "              train_f1 = train_f1_all.mean()\n",
        "\n",
        "              multi_label_test_predictions = np.array(test_predictions)\n",
        "              y_test=np.empty([2500,multi])\n",
        "              for i in range(len(data['y_test'])):\n",
        "                y_test[i][:]=data['y_test'][i]\n",
        "\n",
        "              test_precision_all = np.empty([multi])\n",
        "              test_recall_all = np.empty([multi])\n",
        "              test_f1_all = np.empty([multi])\n",
        "              for i in range(len(test_predictions[0])):\n",
        "                  test_precision_all[i],test_recall_all[i],test_f1_all[i],_ = precision_recall_fscore_support(multi_label_test_predictions[:][i],y_test[:][i],average='binary')\n",
        "              test_precision = test_precision_all.mean()\n",
        "              test_recall = test_recall_all.mean()\n",
        "              test_f1 = test_f1_all.mean()\n",
        "              print(\"Epoch: %d, loss: %.5f, Train precision: %.5f,Train recall: %.5f,Train f1: %.5f, Test precision: %.5f, Test recall: %.5f,Test f1: %.5f\" % (epoch+1, loss.item(), train_precision,train_recall,train_f1,test_precision,test_recall,test_f1))\n",
        "            \n",
        "    @staticmethod\n",
        "    def evaluation(model, loader_test):\n",
        "        \n",
        "        # Set the model in evaluation mode\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        \n",
        "        # Starst evaluation phase\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in loader_test:\n",
        "                y_pred = model(x_batch)\n",
        "                predictions += list(y_pred.detach().numpy())\n",
        "        return predictions\n",
        "        \n",
        "    @staticmethod\n",
        "    def calculate_accuray(grand_truth, predictions):\n",
        "        # Metrics calculation\n",
        "        true_positives = 0\n",
        "        true_negatives = 0 \n",
        "        for true, pred in zip(grand_truth, predictions):\n",
        "            if (pred >= 0.5) and (true == 1):\n",
        "                true_positives += 1\n",
        "            elif (pred < 0.5) and (true == 0):\n",
        "                true_negatives += 1\n",
        "            else:\n",
        "                pass\n",
        "        # Return accuracy\n",
        "        return (true_positives+true_negatives) / len(grand_truth)"
      ],
      "metadata": {
        "id": "Vh6LioohHGvm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(num_words, seq_len, target, binary=True):\n",
        "  # Preprocessing pipeline\n",
        "  pr = Preprocessing(num_words, seq_len, target, binary)\n",
        "  pr.load_data(binary)\n",
        "  pr.text_tokenization()\n",
        "  pr.build_vocabulary()\n",
        "  pr.word_to_idx()\n",
        "  pr.padding_sentences()\n",
        "  pr.split_data()\n",
        "\n",
        "  return {'x_train': pr.x_train, 'y_train': pr.y_train, 'x_test': pr.x_test, 'y_test': pr.y_test}"
      ],
      "metadata": {
        "id": "rbwh_HI5pq2G"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_data=prepare_data(45173, 2200, 'rolled_up_code_584', binary=True)"
      ],
      "metadata": {
        "id": "yLAJMyqgg98k"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_label_data = prepare_data(45173, 2200, 'rolled_up_code_518', binary=False)"
      ],
      "metadata": {
        "id": "9RmiGaQmhDDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_BoT=BoT()\n",
        "Run().train(model_BoT, binary_data, len(rolled_up_codes_list), binary=True)"
      ],
      "metadata": {
        "id": "KHyU9Hr6oDZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1815779-58c6-45df-a498-c9389f71fbbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, loss: 0.68180, Train precision: 0.16633,Train recall: 0.72671,Train f1: 0.27071, Test accuracy: 0.60960,Test precision: 0.36878, Test recall: 0.65714,Test f1: 0.47243\n",
            "Epoch: 2, loss: 0.63790, Train precision: 0.52488,Train recall: 0.69061,Train f1: 0.59645, Test accuracy: 0.66400,Test precision: 0.51814, Test recall: 0.69536,Test f1: 0.59381\n",
            "Epoch: 3, loss: 0.59362, Train precision: 0.57066,Train recall: 0.76956,Train f1: 0.65535, Test accuracy: 0.71520,Test precision: 0.52068, Test recall: 0.81078,Test f1: 0.63412\n",
            "Epoch: 4, loss: 0.53793, Train precision: 0.57976,Train recall: 0.85350,Train f1: 0.69048, Test accuracy: 0.73520,Test precision: 0.53502, Test recall: 0.85101,Test f1: 0.65699\n",
            "Epoch: 5, loss: 0.48162, Train precision: 0.59710,Train recall: 0.87500,Train f1: 0.70982, Test accuracy: 0.74280,Test precision: 0.55274, Test recall: 0.85286,Test f1: 0.67076\n",
            "Epoch: 6, loss: 0.43273, Train precision: 0.61530,Train recall: 0.87825,Train f1: 0.72362, Test accuracy: 0.75200,Test precision: 0.57131, Test recall: 0.85805,Test f1: 0.68592\n",
            "Epoch: 7, loss: 0.39143, Train precision: 0.62980,Train recall: 0.88423,Train f1: 0.73564, Test accuracy: 0.75880,Test precision: 0.58312, Test recall: 0.86375,Test f1: 0.69622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_BoT_mul=BoT_mul()\n",
        "print(len(rolled_up_codes_list))\n",
        "Run().train(model_BoT_mul, multi_label_data, len(rolled_up_codes_list), binary=False)"
      ],
      "metadata": {
        "id": "BtMK5djzeUE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN_baseline=CNN_baseline()\n",
        "Run().train(model_CNN_baseline, binary_data, len(rolled_up_codes_list), binary=True)"
      ],
      "metadata": {
        "id": "A9NGBxMFeUOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN_baseline_mul=CNN_baseline_mul()\n",
        "Run().train(model_CNN_baseline_mul, multi_label_data, len(rolled_up_codes_list), binary=False)"
      ],
      "metadata": {
        "id": "ibgzHZZReUVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN_Three_Conv_multi = CNN_Three_Conv_multi()\n",
        "Run().train(model_CNN_Three_Conv_multi, multi_label_data, len(rolled_up_codes_list), binary=False)"
      ],
      "metadata": {
        "id": "NDR15DMRu78C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "model_without_cat_rolled_up.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}