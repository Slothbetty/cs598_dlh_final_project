{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models_cat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bzhao10/cs598_dlh_final_project/blob/main/models_cat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAgJKsPOQGdy",
        "outputId": "6aa347c0-7a2f-4599-9186-631cd0b74700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "notes=pd.read_csv('/content/drive/MyDrive/DLH_final_project_dataset/preprocessed_reg_icd_codes_10000.csv')\n",
        "notes.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zveoIxFQQN9o",
        "outputId": "f0675edc-9a00-4282-dbff-a86f630a969c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'TEXT', 'reg_code_00845', 'reg_code_0088',\n",
              "       'reg_code_0090', 'reg_code_0091', 'reg_code_01085', 'reg_code_0311',\n",
              "       'reg_code_0380', 'reg_code_03811',\n",
              "       ...\n",
              "       'category_Case Management', 'category_Consult', 'category_General',\n",
              "       'category_Nursing/other', 'category_Nutrition', 'category_Pharmacy',\n",
              "       'category_Physician', 'category_Rehab Services', 'category_Respiratory',\n",
              "       'category_Social Work'],\n",
              "      dtype='object', length=1166)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_codes_list = notes.loc[:, notes.columns.str.startswith('reg_code_')].columns.tolist()\n",
        "print(len(reg_codes_list))\n"
      ],
      "metadata": {
        "id": "CaQyU-2HiT6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rolled_up_codes_list=['rolled_up_code_518','rolled_up_code_427','rolled_up_code_584','rolled_up_code_428','rolled_up_code_276','rolled_up_code_401','rolled_up_code_285','rolled_up_code_414','rolled_up_code_272','rolled_up_code_038']"
      ],
      "metadata": {
        "id": "MDscFnWujoj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.value_counts(notes.rolled_up_code_518)"
      ],
      "metadata": {
        "id": "mx8qcDeFcdoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "note.to_csv('/content/drive/MyDrive/DLH_final_project_dataset/preprocessed_rolled_up_icd_codes_10000.csv')"
      ],
      "metadata": {
        "id": "UhoOpW5Dib2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add category\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "class Preprocessing:\n",
        "  def __init__(self, num_words, seq_len,category,target,binary=True):\n",
        "    self.data = '/content/drive/MyDrive/DLH_final_project_dataset/preprocessed_reg_icd_codes_10000.csv'\n",
        "    self.num_words = num_words\n",
        "    self.seq_len = seq_len\n",
        "    self.vocabulary = None\n",
        "    self.x_tokenized = None\n",
        "    self.x_padded = None\n",
        "    self.x_raw = None\n",
        "    self.y = None\n",
        "    \n",
        "    self.x_train = None\n",
        "    self.x_test = None\n",
        "    self.y_train = None\n",
        "    self.y_test = None\n",
        "    \n",
        "    self.target=target\n",
        "    self.category=category\n",
        "  def load_data(self,binary):\n",
        "    # Reads the raw csv file and split into\n",
        "    # sentences (x) and target (y)\n",
        "    df = pd.read_csv(self.data)\n",
        "    #df.drop(['id','keyword','location'], axis=1, inplace=True)\n",
        "    self.x_raw = df['TEXT'].values\n",
        "    if (binary==False):\n",
        "      #df['rolled_up_codes'] = df.loc[:, 'rolled_up_code_008':'rolled_up_code_V88'].values.tolist()\n",
        "      #rolled_up_codes_list=['rolled_up_code_518','rolled_up_code_427','rolled_up_code_584','rolled_up_code_428','rolled_up_code_276',\n",
        "                            #'rolled_up_code_401','rolled_up_code_285','rolled_up_code_414','rolled_up_code_272','rolled_up_code_038']\n",
        "      #df['rolled_up_codes'] = df.loc[:, rolled_up_codes_list].values.tolist()\n",
        "      #rolled_up_codes_list = notes.loc[:, notes.columns.str.startswith('rolled_up_code_')].columns.tolist()\n",
        "      #df['rolled_up_codes'] = df.loc[:, rolled_up_codes_list[0]:rolled_up_codes_list[len(rolled_up_codes_list)-1]].values.tolist()\n",
        "      #self.y = df['rolled_up_codes'].values\n",
        "      reg_codes_list = notes.loc[:, notes.columns.str.startswith('reg_code_')].columns.tolist()\n",
        "      df['reg_codes'] = df.loc[:, reg_codes_list[0]:reg_codes_list[len(reg_codes_list)-1]].values.tolist()\n",
        "      self.y = df['reg_codes'].values\n",
        "    else:\n",
        "      self.y = df[self.target].values\n",
        "   \n",
        "    self.cat=df[self.category].values\n",
        "  '''\n",
        "  def clean_text(self):\n",
        "    # Removes special symbols and just keep\n",
        "    # words in lower or upper form\n",
        "    self.x_raw = [x.lower() for x in self.x_raw]\n",
        "    self.x_raw = [re.sub(r'[^A-Za-z]+', ' ', x) for x in self.x_raw]\n",
        "  '''  \n",
        "  def text_tokenization(self):\n",
        "    # Tokenizes each sentence by implementing the nltk tool\n",
        "    self.x_raw = [word_tokenize(x) for x in self.x_raw]\n",
        "    \n",
        "  def build_vocabulary(self):\n",
        "    # Builds the vocabulary and keeps the \"x\" most frequent word\n",
        "    self.vocabulary = dict()\n",
        "    fdist = nltk.FreqDist()\n",
        "    \n",
        "    for sentence in self.x_raw:\n",
        "      for word in sentence:\n",
        "        fdist[word] += 1\n",
        "        \n",
        "    common_words = fdist.most_common(self.num_words)\n",
        "    \n",
        "    for idx, word in enumerate(common_words):\n",
        "      self.vocabulary[word[0]] = (idx+1)\n",
        "  \n",
        "  def word_to_idx(self):\n",
        "    # By using the dictionary (vocabulary), it is transformed\n",
        "    # each token into its index based representatio\n",
        "    self.x_tokenized = list() \n",
        "    \n",
        "    for sentence in self.x_raw:\n",
        "      temp_sentence = list()\n",
        "      for word in sentence:\n",
        "        if word in self.vocabulary.keys():\n",
        "          temp_sentence.append(self.vocabulary[word])\n",
        "      self.x_tokenized.append(temp_sentence)\n",
        "      \n",
        "  def padding_sentences(self):\n",
        "    # Each sentence which does not fulfill the required le\n",
        "    # it's padded with the index 0\n",
        "    pad_idx = 0\n",
        "    self.x_padded = list()\n",
        "    \n",
        "    for sentence in self.x_tokenized:\n",
        "      while len(sentence) < self.seq_len:\n",
        "        sentence.insert(len(sentence), pad_idx)\n",
        "      self.x_padded.append(sentence)\n",
        " \n",
        "    self.x_padded = np.array(self.x_padded)\n",
        "    self.cat=self.cat.reshape(-1,1)\n",
        "    self.x_padded=np.append(self.x_padded,self.cat,1)\n",
        "  def split_data(self):\n",
        "    self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x_padded, self.y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "bcQTOu75iofL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(num_words, seq_len,category,target,binary=True):\n",
        "  # Preprocessing pipeline\n",
        "  pr = Preprocessing(num_words, seq_len,category,target,binary)\n",
        "  pr.load_data(binary)\n",
        "  #pr.clean_text()\n",
        "  pr.text_tokenization()\n",
        "  pr.build_vocabulary()\n",
        "  pr.word_to_idx()\n",
        "  pr.padding_sentences()\n",
        "  pr.split_data()\n",
        "\n",
        "  return {'x_train': pr.x_train, 'y_train': pr.y_train, 'x_test': pr.x_test, 'y_test': pr.y_test}\n",
        "\n",
        "#data=prepare_data(45173, 2200,'category_Nursing/other','rolled_up_code_038',binary=True)\n",
        "data=prepare_data(45173, 2200,'category_Respiratory','rolled_up_code_518',binary=False)"
      ],
      "metadata": {
        "id": "KwYQ8d4_QlwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data['y_test'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXmfM2seir7F",
        "outputId": "6c959248-270b-4b15-d2d7-ccf6f6bfa51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1154"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add category for Bot model for binary prediction\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class BOT_cat(nn.ModuleList):\n",
        "\n",
        "   def __init__(self, seq_len=2200,num_words=45173,embedding_size=64,out_size=32):\n",
        "      super(BOT_cat, self).__init__()\n",
        "\n",
        "      # Parameters regarding text preprocessing\n",
        "      self.seq_len = seq_len\n",
        "      self.num_words = num_words\n",
        "      self.embedding_size = embedding_size\n",
        "  \n",
        "      # Kernel sizes\n",
        "      self.kernel_1 = 3\n",
        "   \n",
        "      # Output size for each convolution\n",
        "      self.out_size = out_size\n",
        "      \n",
        "      # Embedding layer definition\n",
        "      self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
        "      \n",
        "      # Fully connected layer definition\n",
        "      # Change based on situation:\n",
        "      # binary classification\n",
        "      self.fc = nn.Linear(65, 1)\n",
        "\n",
        "\n",
        "   def forward(self, x):\n",
        "      report=x[:,:-1]\n",
        "      cat=torch.tensor(x[:,-1])\n",
        "      cat=cat.reshape(-1,1)\n",
        "      x = self.embedding(report)\n",
        " \n",
        "\n",
        "      #x = [batch size, seq. length, hidden dim]\n",
        "      \n",
        "      x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze(1) #averages word vectors across whole sequence length\n",
        "      #x = [batch size, hidden dim]\n",
        "      x=torch.cat((x,cat),1)\n",
        "      x = self.fc(x)\n",
        "      #x = [batch size, output dim]\n",
        "\n",
        "      # Activation function is applied\n",
        "      x = torch.sigmoid(x)\n",
        "      \n",
        "      return x.squeeze()"
      ],
      "metadata": {
        "id": "dwpiiMrZfZyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add category for Bot model for multiple prediction\n",
        "class BOT_cat_mul(nn.ModuleList):\n",
        "\n",
        "   def __init__(self, seq_len=2200,num_words=45173,labels=1154,embedding_size=64,out_size=32):\n",
        "      super(BOT_cat_mul, self).__init__()\n",
        "\n",
        "      # Parameters regarding text preprocessing\n",
        "      self.seq_len = seq_len\n",
        "      self.num_words = num_words\n",
        "      self.embedding_size = embedding_size\n",
        "  \n",
        "      # Kernel sizes\n",
        "      self.kernel_1 = 3\n",
        "   \n",
        "      # Output size for each convolution\n",
        "      self.out_size = out_size\n",
        "      \n",
        "      # Embedding layer definition\n",
        "      self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
        "      \n",
        "      # Fully connected layer definition\n",
        "      # Change based on situation:\n",
        "      # binary classification\n",
        "      # self.fc = nn.Linear(64, 1)\n",
        "      # multi-label classification\n",
        "      self.fc = nn.Linear(65, labels)\n",
        "\n",
        "   def forward(self, x):\n",
        "\n",
        "      report=x[:,:-1]\n",
        "      cat=torch.tensor(x[:,-1])\n",
        "\n",
        "      cat=cat.reshape(-1,1)\n",
        "\n",
        "      x = self.embedding(report)\n",
        "      #x = [batch size, seq. length, hidden dim]\n",
        "      x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze(1) #averages word vectors across whole sequence length\n",
        "      #x = [batch size, hidden dim]\n",
        "\n",
        "      x=torch.cat((x,cat),1)\n",
        "      x = self.fc(x)\n",
        "      #x = [batch size, output dim]\n",
        "\n",
        "      # Activation function is applied\n",
        "      x = torch.sigmoid(x)\n",
        "      \n",
        "      return x.squeeze()"
      ],
      "metadata": {
        "id": "86oXf_xMRuAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add category for CNN baseline model for binary prediction\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class CNN_baseline_cat(nn.ModuleList):\n",
        "\n",
        "   def __init__(self, seq_len=2200,num_words=45173,embedding_size=64,out_size=32):\n",
        "      super(CNN_baseline_cat, self).__init__()\n",
        "\n",
        "      # Parameters regarding text preprocessing\n",
        "      self.seq_len = seq_len\n",
        "      self.num_words = num_words\n",
        "      self.embedding_size = embedding_size\n",
        "      \n",
        "      # CNN parameters definition\n",
        "      # Kernel sizes\n",
        "      self.kernel_1 = 3\n",
        "\n",
        "      \n",
        "      # Output size for each convolution\n",
        "      self.out_size = out_size\n",
        "      \n",
        "      # Embedding layer definition\n",
        "      self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
        "      \n",
        "      # Convolution layers definition\n",
        "      self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1)\n",
        "      \n",
        "      # Max pooling layers definition\n",
        "      self.pool_1 = nn.MaxPool1d(self.kernel_1)\n",
        "      \n",
        "      # Fully connected layer definition\n",
        "      self.fc = nn.Linear(641, 1)\n",
        " \n",
        "   def forward(self, x):\n",
        "      \n",
        "      # Sequence of tokes is filterd through an embedding layer\n",
        "      report=x[:,:-1]\n",
        "      cat=torch.tensor(x[:,-1])\n",
        "\n",
        "      cat=cat.reshape(-1,1)\n",
        "\n",
        "\n",
        "      x = self.embedding(report)\n",
        "\n",
        "      #x=torch.cat((x,cat),1)\n",
        "      # Convolution layer 1 is applied\n",
        "      x1 = self.conv_1(x)\n",
        "      x1 = torch.relu(x1)\n",
        "      x1 = self.pool_1(x1)\n",
        "\n",
        "      # The output of each convolutional layer is concatenated into a unique vector\n",
        "      x1=torch.flatten(x1, 1)\n",
        "\n",
        "      x1=torch.cat((x1,cat),1)\n",
        "      # The \"flattened\" vector is passed through a fully connected layer\n",
        "      out = self.fc(x1)\n",
        "      # Activation function is applied\n",
        "      out = torch.sigmoid(out)\n",
        "      \n",
        "      return out.squeeze()"
      ],
      "metadata": {
        "id": "VTx4mC8DaCVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_baseline_cat_mul(nn.ModuleList):\n",
        "\n",
        "   def __init__(self, seq_len=2200,num_words=45173,labels=1154,embedding_size=64,out_size=32):\n",
        "      super(CNN_baseline_cat_mul, self).__init__()\n",
        "\n",
        "      # Parameters regarding text preprocessing\n",
        "      self.seq_len = seq_len\n",
        "      self.num_words = num_words\n",
        "      self.embedding_size = embedding_size\n",
        "      \n",
        "      # CNN parameters definition\n",
        "      # Kernel sizes\n",
        "      self.kernel_1 = 3\n",
        "\n",
        "      \n",
        "      # Output size for each convolution\n",
        "      self.out_size = out_size\n",
        "      \n",
        "      # Embedding layer definition\n",
        "      self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
        "      \n",
        "      # Convolution layers definition\n",
        "      self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1)\n",
        "      \n",
        "      # Max pooling layers definition\n",
        "      self.pool_1 = nn.MaxPool1d(self.kernel_1)\n",
        "      \n",
        "      # Fully connected layer definition\n",
        "      self.fc = nn.Linear(641, labels)\n",
        " \n",
        "   def forward(self, x):\n",
        "      \n",
        "      # Sequence of tokes is filterd through an embedding layer\n",
        "      \n",
        "      report=x[:,:-1]\n",
        "      cat=torch.tensor(x[:,-1])\n",
        "      cat=cat.reshape(-1,1)\n",
        "      \n",
        "      x = self.embedding(report)\n",
        "\n",
        "      #x=torch.cat((x,cat),1)\n",
        "      # Convolution layer 1 is applied\n",
        "      x1 = self.conv_1(x)\n",
        "      x1 = torch.relu(x1)\n",
        "      x1 = self.pool_1(x1)\n",
        "\n",
        "      \n",
        "      # The output of each convolutional layer is concatenated into a unique vector\n",
        "      x1=torch.flatten(x1, 1)\n",
        "      x1=torch.cat((x1,cat),1)\n",
        "      # The \"flattened\" vector is passed through a fully connected layer\n",
        "      out = self.fc(x1)\n",
        "      # Activation function is applied\n",
        "      out = torch.sigmoid(out)\n",
        "      \n",
        "      return out.squeeze()"
      ],
      "metadata": {
        "id": "LdbdC_wzVR7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DatasetMaper(Dataset):\n",
        "\n",
        "\tdef __init__(self, x, y):\n",
        "\t\tself.x = x\n",
        "\t\tself.y = y\n",
        "\t\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.x)\n",
        "\t\t\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "tthX_TrLS28m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Run:\n",
        "    '''Training, evaluation and metrics calculation'''\n",
        "\n",
        "    @staticmethod\n",
        "    def train(model, data, multi=10,binary=True):\n",
        "        \n",
        "        # Initialize dataset maper\n",
        "        train = DatasetMaper(data['x_train'], data['y_train'])\n",
        "        test = DatasetMaper(data['x_test'], data['y_test'])\n",
        "        \n",
        "        # Initialize loaders\n",
        "        loader_train = DataLoader(train, batch_size=32)\n",
        "        loader_test = DataLoader(test, batch_size=32)\n",
        "        \n",
        "        # Define optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        \n",
        "        # Starts training phase\n",
        "        for epoch in range(15):\n",
        "            # Set model in training model\n",
        "            model.train()\n",
        "            predictions = []\n",
        "            # Starts batch training\n",
        "            for x_batch, y_batch in loader_train:\n",
        "                if (binary==False):\n",
        "                  y_batch = torch.stack(y_batch).T\n",
        "                y_batch = y_batch.type(torch.FloatTensor)\n",
        "                \n",
        "                # Feed the model\n",
        "                y_pred = model(x_batch)\n",
        "\n",
        "                # Loss calculation\n",
        "                loss = F.binary_cross_entropy(y_pred, y_batch)\n",
        "                \n",
        "                # Clean gradientes\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # Gradients calculation\n",
        "                loss.backward()\n",
        "                \n",
        "                # Gradients update\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Save predictions\n",
        "                predictions += list(y_pred.detach().numpy())\n",
        "            \n",
        "            # Evaluation phase\n",
        "            test_predictions = Run.evaluation(model, loader_test)\n",
        "            if (binary==True):\n",
        "              predictions=[1 if pred>0.5 else 0 for pred in predictions]\n",
        "\n",
        "              test_predictions=[1 if pred>0.5 else 0 for pred in test_predictions]\n",
        "\n",
        "              train_precision,train_recall,train_f1,_ = precision_recall_fscore_support(predictions,data['y_train'],average='binary')\n",
        "              test_precision,test_recall,test_f1,_ = precision_recall_fscore_support(test_predictions,data['y_test'],average='binary')\n",
        "              test_accuracy=accuracy_score(data['y_test'],test_predictions)\n",
        "              print(\"Epoch: %d, loss: %.5f, Train precision: %.5f,Train recall: %.5f,Train f1: %.5f, Test accuracy: %.5f,Test precision: %.5f, Test recall: %.5f,Test f1: %.5f\" % (epoch+1, loss.item(), train_precision,train_recall,train_f1,test_accuracy,test_precision,test_recall,test_f1))\n",
        "            else:\n",
        "              for i in range(len(predictions)):\n",
        "                predictions[i]=[1 if pred>0.5 else 0 for pred in predictions[i]]  \n",
        "\n",
        "              for i in range(len(test_predictions)):\n",
        "                test_predictions[i]=[1 if pred>0.5 else 0 for pred in test_predictions[i]] \n",
        "              \n",
        "              multi_label_predictions = np.array(predictions)\n",
        "              print(multi_label_predictions.shape)\n",
        "              y_train=np.empty([7500,multi])\n",
        "              for i in range(len(data['y_train'])):\n",
        "                y_train[i][:]=data['y_train'][i]\n",
        "\n",
        "              train_precision_all = np.empty([multi])\n",
        "              train_recall_all = np.empty([multi])\n",
        "              train_f1_all = np.empty([multi])\n",
        "              for i in range(len(predictions[0])):\n",
        "                  train_precision_all[i],train_recall_all[i],train_f1_all[i],_ = precision_recall_fscore_support(multi_label_predictions[:][i],y_train[:][i],average='binary')\n",
        "              train_precision = train_precision_all.mean()\n",
        "              train_recall = train_recall_all.mean()\n",
        "              train_f1 = train_f1_all.mean()\n",
        "\n",
        "              multi_label_test_predictions = np.array(test_predictions)\n",
        "              print(multi_label_test_predictions.shape)\n",
        "              y_test=np.empty([2500,multi])\n",
        "              for i in range(len(data['y_test'])):\n",
        "                y_test[i][:]=data['y_test'][i]\n",
        "\n",
        "              test_precision_all = np.empty([multi])\n",
        "              test_recall_all = np.empty([multi])\n",
        "              test_f1_all = np.empty([multi])\n",
        "              for i in range(len(test_predictions[0])):\n",
        "                  test_precision_all[i],test_recall_all[i],test_f1_all[i],_ = precision_recall_fscore_support(multi_label_test_predictions[:][i],y_test[:][i],average='binary')\n",
        "              test_precision = test_precision_all.mean()\n",
        "              test_recall = test_recall_all.mean()\n",
        "              test_f1 = test_f1_all.mean()\n",
        "              print(\"Epoch: %d, loss: %.5f, Train precision: %.5f,Train recall: %.5f,Train f1: %.5f, Test precision: %.5f, Test recall: %.5f,Test f1: %.5f\" % (epoch+1, loss.item(), train_precision,train_recall,train_f1,test_precision,test_recall,test_f1))\n",
        "            \n",
        "    @staticmethod\n",
        "    def evaluation(model, loader_test):\n",
        "        \n",
        "        # Set the model in evaluation mode\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        \n",
        "        # Starst evaluation phase\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in loader_test:\n",
        "                y_pred = model(x_batch)\n",
        "                predictions += list(y_pred.detach().numpy())\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "Vh6LioohHGvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bot=BOT_cat()\n",
        "Run().train(model_bot,data)"
      ],
      "metadata": {
        "id": "MIys_2UoWNie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn=CNN_baseline_cat()\n",
        "Run().train(model_cnn,data)"
      ],
      "metadata": {
        "id": "7LobEsg-R9gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bot_mul=BOT_cat_mul()\n",
        "Run().train(model_bot_mul,data,multi=1154,binary=False)"
      ],
      "metadata": {
        "id": "ON0patUEj2XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn_mul=CNN_baseline_cat_mul()\n",
        "Run().train(model_cnn_mul,data,multi=1154,binary=False)"
      ],
      "metadata": {
        "id": "H7rN1r1eYCwT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}